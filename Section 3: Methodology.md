# Section 3: Methodology

### Data Preprocessing
- Handled missing values and formatted date-time fields.
- Scaled numeric features and one-hot encoded categorical variables.
```python
from sklearn.preprocessing import LabelEncoder
import pandas as pd

# T·∫°o b·∫£n sao ƒë·ªÉ x·ª≠ l√Ω tr√°nh thay ƒë·ªïi plan_df g·ªëc
df = plan_df.copy()

# ƒê·∫£m b·∫£o c·ªôt 'air_distance_miles' ·ªü d·∫°ng s·ªë, l·ªói s·∫Ω b·ªã chuy·ªÉn th√†nh NaN
df['air_distance_miles'] = pd.to_numeric(df['air_distance_miles'], errors='coerce')

# Lo·∫°i b·ªè c√°c d√≤ng c√≥ missing ·ªü c·ªôt quan tr·ªçng
df.dropna(subset=['planned_flight_fuel_kilograms', 'air_distance_miles'], inplace=True)

# Ki·ªÉm tra xem 2 c·ªôt c·∫ßn label encode c√≥ t·ªìn t·∫°i kh√¥ng
if 'departure_airport' in df.columns and 'arrival_airport' in df.columns:
    # T·∫°o 2 encoder ri√™ng bi·ªát cho t·ª´ng c·ªôt (tr√°nh encode nh·∫ßm nghƒ©a)
    le_departure = LabelEncoder()
    le_arrival = LabelEncoder()
    
    df['departure_encoded'] = le_departure.fit_transform(df['departure_airport'])
    df['arrival_encoded'] = le_arrival.fit_transform(df['arrival_airport'])
else:
    print("‚ö†Ô∏è M·ªôt trong hai c·ªôt 'departure_airport' ho·∫∑c 'arrival_airport' kh√¥ng t·ªìn t·∫°i.")

```

### Implementation
Three base models were tested:
- `LinearRegression`
- `RandomForestRegressor`
- `XGBRegressor`
  
![image](https://github.com/user-attachments/assets/ef618f14-7af6-4742-8637-92a0d88fc2da)

```pythonimport pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# B∆∞·ªõc 1: X√°c ƒë·ªãnh features (X) v√† target (y)
# V√≠ d·ª•: d·ª± ƒëo√°n l∆∞·ª£ng nhi√™n li·ªáu theo kho·∫£ng c√°ch v√† s√¢n bay m√£ h√≥a
X = df[['air_distance_miles', 'departure_encoded', 'arrival_encoded']]
y = df['planned_flight_fuel_kilograms']

# B∆∞·ªõc 2: T√°ch d·ªØ li·ªáu train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# B∆∞·ªõc 3: Kh·ªüi t·∫°o m√¥ h√¨nh
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42, verbosity=0)
}

# B∆∞·ªõc 4: Hu·∫•n luy·ªán v√† ƒë√°nh gi√°
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)

    mae = mean_absolute_error(y_test, preds)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    r2 = r2_score(y_test, preds)

    results.append({
        "Model": name,
        "MAE": mae,
        "RMSE": rmse,
        "R2 Score": r2
    })

# üìÑ B∆∞·ªõc 5: Hi·ªÉn th·ªã k·∫øt qu·∫£
results_df = pd.DataFrame(results)
print(results_df)
```

### Refinement XGBoost
- Used 5-fold cross-validation.
- Performed hyperparameter tuning on XGBoost.
![image](https://github.com/user-attachments/assets/e37e6694-d693-4e39-aa18-cce3f2c5ac7b)

```from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error

# Kh·ªüi t·∫°o m√¥ h√¨nh g·ªëc
xgb = XGBRegressor(random_state=42, verbosity=0)

# ƒê·ªãnh nghƒ©a c√°c tham s·ªë ƒë·ªÉ Grid Search
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1, 0.2]
}

# Thi·∫øt l·∫≠p GridSearchCV v·ªõi 5-fold cross-validation v√† MAE l√†m ti√™u ch√≠ ƒë√°nh gi√°
grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=1
)

# Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n t·∫≠p hu·∫•n luy·ªán
grid_search.fit(X_train, y_train)

# L·∫•y ra m√¥ h√¨nh t·ªët nh·∫•t v√† c√°c tham s·ªë t∆∞∆°ng ·ª©ng
best_xgb = grid_search.best_estimator_
print("‚úÖ Best XGBoost Parameters:", grid_search.best_params_)

# D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra v√† ƒë√°nh gi√°
y_pred = best_xgb.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f"‚úÖ MAE of Best Model on Test Set: {mae:.2f}")

```

### Stacking model (XGBoost + LinearRegression)
Then, a **stacking model** was built:

![image](https://github.com/user-attachments/assets/36659f1c-1878-493c-85bc-771a283595b4)

```python
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import StackingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Kh·ªüi t·∫°o m√¥ h√¨nh Stacking Regressor
stack_model = StackingRegressor(
    estimators=[
        ('xgb', best_xgb),
        ('lr', LinearRegression())
    ],
    final_estimator=LinearRegression(),  # meta-model
    cv=5,
    n_jobs=-1,
    passthrough=False
)

# Hu·∫•n luy·ªán m√¥ h√¨nh
stack_model.fit(X_train, y_train)

# D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra
predictions = stack_model.predict(X_test)

# ƒê√°nh gi√° hi·ªáu su·∫•t
mae = mean_absolute_error(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
r2 = r2_score(y_test, predictions)

# In k·∫øt qu·∫£
print("üìä Stacking Regressor Performance:")
print(f"‚úÖ MAE : {mae:.2f} kg")
print(f"‚úÖ RMSE: {rmse:.2f} kg")
print(f"‚úÖ R¬≤   : {r2:.4f}")
```
