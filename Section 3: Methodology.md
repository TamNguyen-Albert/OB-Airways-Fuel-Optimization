## Section 3: Methodology

### Data Preprocessing
- Joined actual_flights and flight_plan using `flight_id`.
- Handled missing values and formatted date-time fields.
- Scaled numeric features and one-hot encoded categorical variables.
```python
from sklearn.preprocessing import LabelEncoder
import pandas as pd

# âœ… Táº¡o báº£n sao Ä‘á»ƒ xá»­ lÃ½ trÃ¡nh thay Ä‘á»•i plan_df gá»‘c
df = plan_df.copy()

# âœ… Äáº£m báº£o cá»™t 'air_distance_miles' á»Ÿ dáº¡ng sá»‘, lá»—i sáº½ bá»‹ chuyá»ƒn thÃ nh NaN
df['air_distance_miles'] = pd.to_numeric(df['air_distance_miles'], errors='coerce')

# âœ… Loáº¡i bá» cÃ¡c dÃ²ng cÃ³ missing á»Ÿ cá»™t quan trá»ng
df.dropna(subset=['planned_flight_fuel_kilograms', 'air_distance_miles'], inplace=True)

# âœ… Kiá»ƒm tra xem 2 cá»™t cáº§n label encode cÃ³ tá»“n táº¡i khÃ´ng
if 'departure_airport' in df.columns and 'arrival_airport' in df.columns:
    # Táº¡o 2 encoder riÃªng biá»‡t cho tá»«ng cá»™t (trÃ¡nh encode nháº§m nghÄ©a)
    le_departure = LabelEncoder()
    le_arrival = LabelEncoder()
    
    df['departure_encoded'] = le_departure.fit_transform(df['departure_airport'])
    df['arrival_encoded'] = le_arrival.fit_transform(df['arrival_airport'])
else:
    print("âš ï¸ Má»™t trong hai cá»™t 'departure_airport' hoáº·c 'arrival_airport' khÃ´ng tá»“n táº¡i.")

```

### Implementation
Three base models were tested:
- `LinearRegression`
- `RandomForestRegressor`
- `XGBRegressor`
  
![image](https://github.com/user-attachments/assets/ef618f14-7af6-4742-8637-92a0d88fc2da)

```pythonimport pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ğŸ§  BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh features (X) vÃ  target (y)
# VÃ­ dá»¥: dá»± Ä‘oÃ¡n lÆ°á»£ng nhiÃªn liá»‡u theo khoáº£ng cÃ¡ch vÃ  sÃ¢n bay mÃ£ hÃ³a
X = df[['air_distance_miles', 'departure_encoded', 'arrival_encoded']]
y = df['planned_flight_fuel_kilograms']

# ğŸ§ª BÆ°á»›c 2: TÃ¡ch dá»¯ liá»‡u train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ğŸ¤– BÆ°á»›c 3: Khá»Ÿi táº¡o mÃ´ hÃ¬nh
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42, verbosity=0)
}

# ğŸ“Š BÆ°á»›c 4: Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)

    mae = mean_absolute_error(y_test, preds)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    r2 = r2_score(y_test, preds)

    results.append({
        "Model": name,
        "MAE": mae,
        "RMSE": rmse,
        "R2 Score": r2
    })

# ğŸ“„ BÆ°á»›c 5: Hiá»ƒn thá»‹ káº¿t quáº£
results_df = pd.DataFrame(results)
print(results_df)
```

### Refinement XGBoost
- Used 5-fold cross-validation.
- Performed hyperparameter tuning on XGBoost.
![image](https://github.com/user-attachments/assets/e37e6694-d693-4e39-aa18-cce3f2c5ac7b)

```from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error

# âœ… Khá»Ÿi táº¡o mÃ´ hÃ¬nh gá»‘c
xgb = XGBRegressor(random_state=42, verbosity=0)

# âœ… Äá»‹nh nghÄ©a cÃ¡c tham sá»‘ Ä‘á»ƒ Grid Search
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1, 0.2]
}

# âœ… Thiáº¿t láº­p GridSearchCV vá»›i 5-fold cross-validation vÃ  MAE lÃ m tiÃªu chÃ­ Ä‘Ã¡nh giÃ¡
grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    cv=5,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=1  # Cho biáº¿t quÃ¡ trÃ¬nh cháº¡y (cÃ³ thá»ƒ bá» náº¿u muá»‘n yÃªn láº·ng)
)

# âœ… Huáº¥n luyá»‡n mÃ´ hÃ¬nh trÃªn táº­p huáº¥n luyá»‡n
grid_search.fit(X_train, y_train)

# âœ… Láº¥y ra mÃ´ hÃ¬nh tá»‘t nháº¥t vÃ  cÃ¡c tham sá»‘ tÆ°Æ¡ng á»©ng
best_xgb = grid_search.best_estimator_
print("âœ… Best XGBoost Parameters:", grid_search.best_params_)

# âœ… Dá»± Ä‘oÃ¡n trÃªn táº­p kiá»ƒm tra vÃ  Ä‘Ã¡nh giÃ¡
y_pred = best_xgb.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print(f"âœ… MAE of Best Model on Test Set: {mae:.2f}")

```

### Stacking model (XGBoost + LinearRegression)
Then, a **stacking model** was built:
![image](https://github.com/user-attachments/assets/36659f1c-1878-493c-85bc-771a283595b4)

```python
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import StackingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# âœ… Äáº£m báº£o best_xgb Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n (tá»« GridSearchCV)
# âœ… Äáº£m báº£o X_train, y_train, X_test, y_test Ä‘Ã£ Ä‘Æ°á»£c chuáº©n bá»‹ tá»« trÆ°á»›c

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh Stacking Regressor
stack_model = StackingRegressor(
    estimators=[
        ('xgb', best_xgb),
        ('lr', LinearRegression())
    ],
    final_estimator=LinearRegression(),  # meta-model
    cv=5,
    n_jobs=-1,
    passthrough=False  # Äáº·t True náº¿u muá»‘n truyá»n X gá»‘c vÃ o final_estimator cÃ¹ng vá»›i predictions
)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
stack_model.fit(X_train, y_train)

# Dá»± Ä‘oÃ¡n trÃªn táº­p kiá»ƒm tra
predictions = stack_model.predict(X_test)

# ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t
mae = mean_absolute_error(y_test, predictions)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
r2 = r2_score(y_test, predictions)

# In káº¿t quáº£
print("ğŸ“Š Stacking Regressor Performance:")
print(f"âœ… MAE : {mae:.2f} kg")
print(f"âœ… RMSE: {rmse:.2f} kg")
print(f"âœ… RÂ²   : {r2:.4f}")
```
